<?xml version="1.0" encoding="utf-8"?>
<MyConfig xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <ChatConfig>
    <ConfigPresets>
      <ConfigPreset>
        <Name>Gemma 2</Name>
        <Model>gemma-2-27b-it-Q4_K_M.gguf</Model>
        <TemplateName>Gemma 2</TemplateName>
        <SamplerName>simple-1</SamplerName>
        <SystemPrompt>You are a helpful AI assistant.</SystemPrompt>
        <GpuLayers>12</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>512</BatchSize>
        <ThreadCount>4</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Dolphin dpo laser</Name>
        <Model>dolphin-2.6-mistral-7b-dpo-laser.Q4_0.gguf</Model>
        <TemplateName>ChatML</TemplateName>
        <SamplerName>Divine Intellect 2</SamplerName>
        <SystemPrompt>You are an assisntant. Answer users questions. Use only english language.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>15360</ContextLength>
        <BatchSize>2048</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Dolphin mistral - grammar</Name>
        <Model>dolphin-2.6-mistral-7b.Q4_K_M.gguf</Model>
        <TemplateName>ChatML-grmmar</TemplateName>
        <SamplerName>simple-1</SamplerName>
        <SystemPrompt />
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>1204</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Dolphin Phi-2</Name>
        <Model>dolphin-2_6-phi-2.Q5_K_M.gguf</Model>
        <TemplateName>Phi2</TemplateName>
        <SamplerName>simple-1</SamplerName>
        <SystemPrompt>You are an assisntant. Answer users questions.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>2048</ContextLength>
        <BatchSize>512</BatchSize>
        <ThreadCount>4</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>DeepSeek 1.5</Name>
        <Model>deepseek-coder-7b-instruct-v1.5-Q4_K_M.gguf</Model>
        <TemplateName>DeepSeek</TemplateName>
        <SamplerName>Coder</SamplerName>
        <SystemPrompt />
        <GpuLayers>100</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>1024</BatchSize>
        <ThreadCount>1</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Dolphin mistral</Name>
        <Model>dolphin-2.6-mistral-7b.Q4_K_M.gguf</Model>
        <TemplateName>ChatML</TemplateName>
        <SamplerName>Divine Intellect</SamplerName>
        <SystemPrompt>You are an assisntant. Answer users questions.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>1024</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>WizardLM-2-7B</Name>
        <Model>WizardLM-2-7B-abliterated-Q5_K_M.gguf</Model>
        <TemplateName>Vicuna</TemplateName>
        <SamplerName>Divine Intellect</SamplerName>
        <SystemPrompt>You are an assisntant. Answer users questions.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>1024</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>true</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Llama 3.1</Name>
        <Model>Meta-Llama-3.1-8B-Instruct-abliterated-Q4_K_M.gguf</Model>
        <TemplateName>Llama 3</TemplateName>
        <SamplerName>Mirostat</SamplerName>
        <SystemPrompt>You are a helpful assistant.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>8192</ContextLength>
        <BatchSize>1024</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>MS Phi-3 mini</Name>
        <Model>Phi-3-mini-4k-instruct-Q4_K_M.gguf</Model>
        <TemplateName>Phi3</TemplateName>
        <SamplerName>simple-1</SamplerName>
        <SystemPrompt>You are a helpful AI assistant.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>512</BatchSize>
        <ThreadCount>4</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Llama 3 - SFR</Name>
        <Model>SFR-Iterative-DPO-LLaMA-3-8B-R-Q4_K_M.gguf</Model>
        <TemplateName>Llama 3</TemplateName>
        <SamplerName>Mirostat</SamplerName>
        <SystemPrompt>You are a helpful assistant.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>8192</ContextLength>
        <BatchSize>1024</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>MS Phi-3 mini 128k</Name>
        <Model>Phi-3-mini-128k-instruct.Q4_K_M.gguf</Model>
        <TemplateName>Phi3</TemplateName>
        <SamplerName>simple-1</SamplerName>
        <SystemPrompt>You are a helpful AI assistant.</SystemPrompt>
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>8192</ContextLength>
        <BatchSize>512</BatchSize>
        <ThreadCount>4</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>true</UseFlashAttention>
      </ConfigPreset>
      <ConfigPreset>
        <Name>Karen</Name>
        <Model>karen_theeditor_v2_creative_mistral_7b.Q4_K_M.gguf</Model>
        <TemplateName>ChatML-Karen</TemplateName>
        <SamplerName>LLaMA-Precise</SamplerName>
        <SystemPrompt />
        <GpuLayers>50</GpuLayers>
        <MainGpu>0</MainGpu>
        <Seed>-1</Seed>
        <ContextLength>4096</ContextLength>
        <BatchSize>1204</BatchSize>
        <ThreadCount>6</ThreadCount>
        <BatchThreadCount>1</BatchThreadCount>
        <UseFlashAttention>false</UseFlashAttention>
      </ConfigPreset>
    </ConfigPresets>
    <PromptTemplates>
      <PromptTemplate>
        <Name>ChatML</Name>
        <System>&lt;|im_start|&gt;system\n{0}&lt;|im_end|&gt;</System>
        <Prompt>\n&lt;|im_start|&gt;user\n{0}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</Prompt>
        <Response>{0}&lt;|im_end|&gt;</Response>
        <ExtraStopTokens>&lt;|im_end|&gt;</ExtraStopTokens>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
      <PromptTemplate>
        <Name>ChatML-grmmar</Name>
        <System>&lt;|im_start|&gt;system\nCorrect and rephrase user text grammar errors to casual English.&lt;|im_end|&gt;\n&lt;|im_start|&gt;Text:\nshe no went to market&lt;|im_end|&gt;\n&lt;|im_start|&gt;Output:\nShe didn't go the the market.&lt;|im_end|&gt;</System>
        <Prompt>\n&lt;|im_start|&gt;Text:\n{0}&lt;|im_end|&gt;\n&lt;|im_start|&gt;Output:\n</Prompt>
        <Response>&lt;|im_end|&gt;</Response>
        <ExtraStopTokens>&lt;|im_end|&gt;</ExtraStopTokens>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
      <PromptTemplate>
        <Name>DeepSeek</Name>
        <System>You are an AI programming assistant, and you answer questions related to computer science.\n</System>
        <Prompt>### Instruction:\n{0}\n### Response:\n</Prompt>
        <Response>{0}\n&lt;|EOT|&gt;\n</Response>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
      <PromptTemplate>
        <Name>Vicuna</Name>
        <System>{0}\n</System>
        <Prompt>USER: {0}\nASSISTANT: </Prompt>
        <Response>{0}\n</Response>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
      <PromptTemplate>
        <Name>Gemma 2</Name>
        <Prompt>&lt;start_of_turn&gt;user\n{0}&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n</Prompt>
        <Response>{0}&lt;end_of_turn&gt;\n</Response>
        <PrependBosToken>true</PrependBosToken>
      </PromptTemplate>
      <PromptTemplate>
        <Name>Llama 3</Name>
        <System>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{0}&lt;|eot_id|&gt;</System>
        <Prompt>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{0}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n</Prompt>
        <Response>{0}&lt;|eot_id|&gt;</Response>
        <ExtraStopTokens>&lt;|eot_id|&gt;</ExtraStopTokens>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
      <PromptTemplate>
        <Name>Phi3</Name>
        <System />
        <Prompt>&lt;|user|&gt;\n{0}&lt;|end|&gt;\n&lt;|assistant|&gt;\n</Prompt>
        <Response>{0}&lt;|end|&gt;\n</Response>
        <ExtraStopTokens>&lt;|end|&gt;, &lt;|assistant|&gt;</ExtraStopTokens>
        <PrependBosToken>true</PrependBosToken>
      </PromptTemplate>
      <PromptTemplate>
        <Name>Phi2</Name>
        <System>System: {0}\n</System>
        <Prompt>User: {0}\n\nAssistant: </Prompt>
        <Response>{0}\n</Response>
        <PrependBosToken>true</PrependBosToken>
      </PromptTemplate>
      <PromptTemplate>
        <Name>ChatML-Karen</Name>
        <System>&lt;|im_start|&gt;system\n&lt;|im_end|&gt;</System>
        <Prompt>\n&lt;|im_start|&gt;user\nEdit the following text for spelling and grammar mistakes: {0} &lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</Prompt>
        <Response>{0}&lt;|im_end|&gt;</Response>
        <ExtraStopTokens>&lt;|im_end|&gt;</ExtraStopTokens>
        <PrependBosToken xsi:nil="true" />
      </PromptTemplate>
    </PromptTemplates>
    <SamplingParams>
      <SamplingParams>
        <Name>Divine Intellect</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>1.31</Temperature>
        <TopK>49</TopK>
        <TopP>0.14</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.17</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>LLaMA-Precise</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.7</Temperature>
        <TopK>40</TopK>
        <TopP>0.1</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.18</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>simple-1</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.7</Temperature>
        <TopK>20</TopK>
        <TopP>0.9</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.15</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Big O</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.87</Temperature>
        <TopK>85</TopK>
        <TopP>0.99</TopP>
        <MinP>0</MinP>
        <TfsZ>0.68</TfsZ>
        <TypicalP>0.68</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.01</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Debug-deterministic</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>1</Temperature>
        <TopK>1</TopK>
        <TopP>1</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>0</PenaltyLastN>
        <PenaltyRepeat>1</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Contrastive Search</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>1</Temperature>
        <TopK>4</TopK>
        <TopP>1</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>0</PenaltyLastN>
        <PenaltyRepeat>1</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Yara</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.82</Temperature>
        <TopK>72</TopK>
        <TopP>0.21</TopP>
        <MinP>0</MinP>
        <TfsZ>0.68</TfsZ>
        <TypicalP>0.68</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.19</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Shortwave</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>1.53</Temperature>
        <TopK>33</TopK>
        <TopP>0.64</TopP>
        <MinP>0.95</MinP>
        <TfsZ>0.68</TfsZ>
        <TypicalP>0.68</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.07</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Divine Intellect 2</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>1.31</Temperature>
        <TopK>49</TopK>
        <TopP>0.14</TopP>
        <MinP>0.05</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.17</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Midnight Enigma</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.98</Temperature>
        <TopK>100</TopK>
        <TopP>0.37</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.18</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Strict MinP</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.7</Temperature>
        <TopK>1</TopK>
        <TopP>1</TopP>
        <MinP>0.1</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1.15</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Coder</Name>
        <ResponseMaxTokenCount>1024</ResponseMaxTokenCount>
        <Temperature>0.1</Temperature>
        <TopK>1</TopK>
        <TopP>1</TopP>
        <MinP>0</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>Disabled</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>0</PenaltyLastN>
        <PenaltyRepeat>1</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
      <SamplingParams>
        <Name>Mirostat</Name>
        <ResponseMaxTokenCount>2048</ResponseMaxTokenCount>
        <Temperature>0.8</Temperature>
        <TopK>40</TopK>
        <TopP>0.95</TopP>
        <MinP>0.05</MinP>
        <TfsZ>1</TfsZ>
        <TypicalP>1</TypicalP>
        <Mirostat>MirostatV2</Mirostat>
        <MirostatTau>5</MirostatTau>
        <MirostatEta>0.1</MirostatEta>
        <PenaltyLastN>64</PenaltyLastN>
        <PenaltyRepeat>1</PenaltyRepeat>
        <PenaltyFreq>0</PenaltyFreq>
        <PenaltyPresent>0</PenaltyPresent>
      </SamplingParams>
    </SamplingParams>
  </ChatConfig>
</MyConfig>